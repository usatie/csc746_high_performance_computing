% \begin{itemize}
%     \item Scaling characteristics of basic and blocked codes. Looking at your two speedup charts, what observations do you make about the relative scalability of these three codes: basic, blocked B4 and blocked B16? What are limits to scalability? How might you modify the codes to reduce the impact of those limitations? 
%     \item Comments about memory system utilization. Looking across the data in Tables 1 and 2 (L2CACHE, L3CACHE), what observations to you make about how well, or not, each of the 4 codes (CBLAS, basic-serial, blocked B4 serial, blocked B16 serial) makes use of the memory system hierarchy? Be clear in your description what you mean by “makes use of the memory system hierarchy” and use data from the Tables to validate your statements.
% \end{itemize}
Overall, Basic OMP scaled well as the level of parallelism increased, particularly due to the large number of iterations in the outermost loop. However, for BMMCO, particularly when the matrix size was small and the block size large, the performance gains from parallelism were limited due to underutilization of threads. In these cases, the number of blocks per row was insufficient to fully leverage the available threads. To mitigate this limitation, we could modify the code by collapsing two outermost loops, increasing the effective number of iterations and thus improving thread utilization.

As shown in Table~\ref{tab:l2-cache} and Table~\ref{tab:l3-cache}, Basic OMP exhibits poor cache utilization, particularly for L1 and L2 caches, when compared to CBLAS. Blocked B16 demonstrates much better L1 cache utilization, approaching the efficiency of CBLAS (only 1.2x to 1.8x more L2 accesses). However, Blocked B16 is still not fully utilizing the L2 cache, as its L3 requests are 3.6x to 4.2x those of CBLAS for larger matrix sizes. Blocked B4 falls between Basic OMP and Blocked B16 in terms of cache efficiency, showing moderate improvement over Basic OMP but still trailing behind Blocked B16 in terms of L1 and L2 cache utilization.

To improve memory system utilization for the blocked methods, increasing the block size helps reduce the number of higher-level cache accesses, but we need to balance this against the number of available blocks and their impact on parallel thread utilization. An optimal balance between block size and parallelism can significantly reduce latency penalties from memory hierarchy inefficiencies, leading to better overall performance.
